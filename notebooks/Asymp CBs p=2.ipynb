{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1639b0a-e7fa-48ae-ac6c-78019a0da82d",
   "metadata": {},
   "source": [
    "# Notebook for Testing different confidence bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55764d12-48a7-4c34-91a4-cd96317f14c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, HTML\n",
    "from scipy import stats\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import special\n",
    "from numpy.linalg import cholesky\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse.linalg import splu\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), \"../src\"))\n",
    "sys.path.append(src_path)\n",
    "from forest_v2 import RegressionTreeModel\n",
    "from forest_v2 import RandomForestModel\n",
    "from forest_v2 import HistogramEstimator\n",
    "import functions as fcts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caa1489b-ff79-4fac-b9ba-ae54f0ff29b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression model\n",
    "# dimension of the feature space\n",
    "p = 2\n",
    "\n",
    "#regression function\n",
    "regression_fct=fcts.m_p2_01\n",
    "\n",
    "# betas for confidence levels 1-\\beta\n",
    "beta=np.array([0.1,0.05,0.01])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db66328e-f99e-4ce3-9b22-eb1e9e3b982c",
   "metadata": {},
   "source": [
    "The section below is used to approximate the quantiles necessary for the CPRF confidence bands. The quantiles for the histogram is below this section. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d44ed-ba61-4ca1-b86b-157ed5b97b7a",
   "metadata": {},
   "source": [
    "# RF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9047bf-1bb3-4103-8df1-836e07d87a0f",
   "metadata": {},
   "source": [
    "## Covariance estimation and covariance matrix construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecc56d20-1693-4b83-b433-645b8095a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth of the trees\n",
    "k = 5 #5,6\n",
    "\n",
    "# smaller k for evaluation grid (choice justified later)\n",
    "k2 = 4 #4,5\n",
    "\n",
    "#Ehrenfest parameters, constant throughout\n",
    "B = 12\n",
    "delta = 7\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "261b2d88-e3fb-4420-a586-a90867dbc110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform V_cap: 0.0163240625 , Ehrenfest V_cap: 0.017990234375 , 2^-k: 0.03125 , 2^-2k: 0.0009765625\n"
     ]
    }
   ],
   "source": [
    "# estimation of all possible covariance entries using the parameters for the partition construction\n",
    "np.random.seed(0)\n",
    "n_parts=50000 # number of split pairs generated\n",
    "\n",
    "# list of all combinations of closeness relations between two points in the feature space\n",
    "comb_list=[]\n",
    "for com in itertools.combinations_with_replacement(range(0, k + 1), p):\n",
    "    comb_list.append(com)\n",
    "combs=np.array(comb_list)\n",
    "\n",
    "#simulation of splits according to the uniform CPRF and calculation of the average intersection volume for all combinations\n",
    "cu_vol=np.zeros(len(combs))\n",
    "for j in range(n_parts):\n",
    "    s1=np.bincount(np.random.choice(p, k, replace=True),minlength=p)\n",
    "    s2=np.bincount(np.random.choice(p, k, replace=True),minlength=p)\n",
    "    for i in range(len(combs)):\n",
    "        cu_vol[i]+=fcts.vol_intersec_2(combs[i],s1,s2)\n",
    "av_vol=cu_vol/n_parts\n",
    "\n",
    "# calculation of estimated covariance from average volumina\n",
    "V_cap_uni=av_vol[len(av_vol)-1]\n",
    "cov_entries_uni=av_vol/V_cap_uni\n",
    "\n",
    "#simulation of splits according to the Ehrenfest CPRF and calculation of the average intersection volume for all combinations\n",
    "cu_vol=np.zeros(len(combs))\n",
    "for j in range(n_parts):\n",
    "    s1=fcts.ehr_splits(np.zeros(p),np.ones(p)*B,delta,k)\n",
    "    s2=fcts.ehr_splits(np.zeros(p),np.ones(p)*B,delta,k)\n",
    "    for i in range(len(combs)):\n",
    "        cu_vol[i]+=fcts.vol_intersec_2(combs[i],s1,s2)\n",
    "av_vol=cu_vol/n_parts\n",
    "\n",
    "# calculation of estimated covariance from average volumina\n",
    "V_cap_ehr=av_vol[len(av_vol)-1]\n",
    "cov_entries_ehr=av_vol/V_cap_ehr\n",
    "\n",
    "print(\"Uniform V_cap:\", V_cap_uni,\", Ehrenfest V_cap:\", V_cap_ehr,\", 2^-k:\", 1/2**k,\", 2^-2k:\",1/2**(2*k))\n",
    "\n",
    "#cov_entries_ehr, cov_entries_uni, #len(cov_entries_uni),special.binom(k+p,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22a9c2c-68a4-46e2-8ba1-2759312cb4fe",
   "metadata": {},
   "source": [
    "Justification for the choice of $k_2$. For any pair of points in the feature space with closeness $(k_2,k_2)$ or larger, the Gaussian process' covariance should be close to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d537cb52-70f1-4680-901f-51f38be986d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest closeness with cov > min_prob, Ehr.: (4, 4) and Uni: (4, 4)\n",
      "Corresponding covariances:  0.9993746607317338 and 0.9964775925110555\n"
     ]
    }
   ],
   "source": [
    "min_prob=0.99 # 0.9\n",
    "print(\"Smallest closeness with cov > min_prob, Ehr.:\",comb_list[np.argmax(cov_entries_ehr>min_prob)], \"and Uni:\", comb_list[np.argmax(cov_entries_uni>min_prob)])\n",
    "print(\"Corresponding covariances: \", cov_entries_ehr[np.argmax(cov_entries_ehr>min_prob)], \"and\",cov_entries_uni[np.argmax(cov_entries_uni>min_prob)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c869038a-0e70-4333-9793-fe373b5ebf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laufzeit: 0.07515311241149902 Sekunden\n",
      "Size of the cov matrix:  (256, 256)\n"
     ]
    }
   ],
   "source": [
    "# Filling in the covariance entries in the matrices\n",
    "X_test=fcts.test_grid(p,k2) # grid for the cov matrices\n",
    "start_time = time.time()\n",
    "X_test_M=np.array(list(itertools.product(X_test, repeat=2)))\n",
    "X_test_V=X_test_M.reshape(2**(k2*p*2),2*p)\n",
    "t_values=np.arange(0,k+1)\n",
    "v_t=(2**t_values).reshape(-1,1)\n",
    "fin_com_cell=np.empty((2**(k2*p*2),p))\n",
    "for i in range(p):\n",
    "    X_temp_1=X_test_V[:,i].reshape(-1,1)\n",
    "    X_temp_2=X_test_V[:,i+p].reshape(-1,1)\n",
    "    ints1=np.dot(X_temp_1,v_t.T).astype(int)\n",
    "    ints2=np.dot(X_temp_2,v_t.T).astype(int)\n",
    "    equal_mask=ints1==ints2\n",
    "    fin_com_cell[:,i] = np.max(np.where(equal_mask, t_values, -np.inf),axis=1)\n",
    "M=fin_com_cell.astype(int)\n",
    "M=np.sort(M,axis=1)\n",
    "M_int=np.sum(M*10**np.arange(p-1,-1,-1),axis=1)\n",
    "int_combs=np.sum(combs*10**np.arange(p-1,-1,-1),axis=1)\n",
    "indices = np.searchsorted(int_combs, M_int)\n",
    "cov_uni=cov_entries_uni[indices].reshape((2**(k2*p),2**(k2*p)))\n",
    "cov_ehr=cov_entries_ehr[indices].reshape((2**(k2*p),2**(k2*p)))\n",
    "end_time = time.time()\n",
    "print(f\"Run time: {end_time - start_time} seconds\")\n",
    "print(\"Size of the cov matrix: \",cov_uni.shape) #, np.info(cov_uni), np.info(cov_ehr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174b9a7b-e94f-4228-a919-f8798b721afc",
   "metadata": {},
   "source": [
    "The next two cells perform the Cholesky decomposition of the adjusted covariance matrices. Either we add a small diagonal matrix or if this is not possible due to the estimation error, we reconstruct the matrix beforehand, based on all eigenvalues greater than $\\epsilon$ and their corresponding eigenvectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fc08bb0-c784-4859-8355-1754792eb321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occured: Matrix is not positive definite. We used reconstruction of the covariance matrix via eigenvectors before the cholesky decomposition.\n"
     ]
    }
   ],
   "source": [
    "epsilon=1e-6\n",
    "try:\n",
    "    L_uni = cholesky(cov_uni+epsilon * np.eye(2**(k2*p)))\n",
    "except Exception as e:\n",
    "    eig_values, eig_vectors = np.linalg.eigh(cov_uni)\n",
    "    corrected_eig_values = np.maximum(eig_values, epsilon)\n",
    "    fixed_cov_uni = eig_vectors @ np.diag(corrected_eig_values) @ eig_vectors.T\n",
    "    L_uni = cholesky(fixed_cov_uni)\n",
    "    print(f\"An error occured: {e}.\", \"We used reconstruction of the covariance matrix via eigenvectors before the cholesky decomposition.\")\n",
    "else:\n",
    "    print(\"We used the cholesky decomposition of the original covariance matrix.\")\n",
    "#stats.describe((L_uni @ L_uni.T)[:,0] -cov_uni[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80412931-f77c-4b62-b918-d8749ac18155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occured: Matrix is not positive definite. We used reconstruction of the covariance matrix via eigenvectors before the cholesky decomposition.\n"
     ]
    }
   ],
   "source": [
    "epsilon=1e-6\n",
    "try:\n",
    "    L_ehr = cholesky(cov_ehr+epsilon * np.eye(2**(k2*p))) #+1e-3 * np.eye(2**(k*p))\n",
    "except Exception as e:\n",
    "    eig_values, eig_vectors = np.linalg.eigh(cov_ehr)\n",
    "    corrected_eig_values = np.maximum(eig_values, epsilon)\n",
    "    fixed_cov_ehr = eig_vectors @ np.diag(corrected_eig_values) @ eig_vectors.T\n",
    "    L_ehr = cholesky(fixed_cov_ehr)\n",
    "    print(f\"An error occured: {e}.\", \"We used reconstruction of the covariance matrix via eigenvectors before the cholesky decomposition.\")\n",
    "else:\n",
    "    print(\"We used the cholesky decomposition of the original covariance matrix.\")\n",
    "#stats.describe((L_ehr @ L_ehr.T)[:,0] -cov_ehr[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598b5db3-c7ae-4de3-83e5-10c5f78861f7",
   "metadata": {},
   "source": [
    "## Quantile estimation of the Gaussian processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db3dbf04-e909-4571-ba26-0b468d8f41eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating suprema of the Gaussian process to estimate the quantiles\n",
    "np.random.seed(0)\n",
    "sups_GP_uni=[]\n",
    "sups_GP_ehr=[]\n",
    "# number of suprema\n",
    "n_sups=100000\n",
    "for _ in range(n_sups):\n",
    "    e=np.random.normal(0,1,2**(k2*p)) #standard gaussian vector\n",
    "    gp_uni = L_uni @ e # multiplication with approriate L to get the correct covariance\n",
    "    gp_ehr = L_ehr @ e\n",
    "    sups_GP_uni.append(np.max(np.abs(gp_uni))) # calculation of the supremum\n",
    "    sups_GP_ehr.append(np.max(np.abs(gp_ehr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93fcb209-3e15-468c-8157-acfcf218815e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantiles at:    [0.9  0.95 0.99]\n",
      "Uniform CPRF:    [3.32514131 3.54033604 3.97369232]\n",
      "Ehrenfest CPRF:  [3.27505637 3.49751455 3.9256088 ]\n"
     ]
    }
   ],
   "source": [
    "#empirical quantiles\n",
    "quants_uni=np.quantile(sups_GP_uni,1-beta)\n",
    "quants_ehr=np.quantile(sups_GP_ehr,1-beta)\n",
    "\n",
    "#stats.describe(sups_GP_uni), stats.describe(sups_GP_ehr)\n",
    "print(\"Quantiles for p = \", p, \" and k = \",k, \" at \",1-beta)\n",
    "print(\"Uniform CPRF:   \",quants_uni)\n",
    "print(\"Ehrenfest CPRF: \", quants_ehr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2044ce82-9755-482f-8de7-7a8ce53c7bd2",
   "metadata": {},
   "source": [
    "## RF CB Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3274d2f7-41ec-4b6d-958a-00f6d2478033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'Simulation results.txt' already existed.\n"
     ]
    }
   ],
   "source": [
    "# if necessary create a text file for the results of the simulations\n",
    "if not os.path.isfile('Simulation results.txt'):\n",
    "    with open('Simulation results.txt', 'w') as f:\n",
    "        f.write( \"Simulation results of asymptotic confidence bands\"+ '\\n')\n",
    "    print(\"The file 'Simulation results.txt' was created.\")\n",
    "else:\n",
    "    print(\"The file 'Simulation results.txt' already existed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce8a001b-7a1f-4923-bf33-3bbaf2128c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training sample size\n",
    "n_samples = 2000 #4000\n",
    "\n",
    "#factor for subsample size\n",
    "r = 0.75  \n",
    "\n",
    "# bandwidth for estimation of sigma by kernel estimator\n",
    "h_g=1/n_samples**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c640cb67-d05c-4d22-b496-1b93e8a2d5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence band radii for n =  2000  p =  2  and k =  5 with confidence levels : [0.9  0.95 0.99]\n",
      "Uniform CPRF:    [0.30398994 0.3236634  0.36328155]\n",
      "Ehrenfest CPRF:  [0.31432011 0.3356703  0.3767562 ]\n"
     ]
    }
   ],
   "source": [
    "# asymptotic standard deviations (pointwise)\n",
    "as_std_uni=np.sqrt(2**(2*k)*V_cap_uni/n_samples)\n",
    "as_std_ehr=np.sqrt(2**(2*k)*V_cap_ehr/n_samples)\n",
    "\n",
    "#confidence band radii based on the quantiles and the standard deviations\n",
    "cb_rad_uni=as_std_uni*quants_uni\n",
    "cb_rad_ehr=as_std_ehr*quants_ehr\n",
    "\n",
    "print(\"Confidence band radii for n = \",n_samples,\" p = \", p, \" and k = \",k, \"with confidence levels :\", 1-beta)\n",
    "print(\"Uniform CPRF:   \",cb_rad_uni)\n",
    "print(\"Ehrenfest CPRF: \", cb_rad_ehr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09845ef8-8ee6-467b-bbcd-4a2fe0a18caf",
   "metadata": {},
   "source": [
    "The next cell is for the simulation of asymptotic confidence bands for the two random forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2e6c00a-6379-49ee-b74d-80d375d13aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grid for supremum calculation for the RF estimators\n",
    "eps=1/2**20\n",
    "g=2**k2\n",
    "splits=np.linspace(0, 1, num=g, endpoint=False)\n",
    "xl=splits+eps\n",
    "xr=splits+1/g-eps\n",
    "axis_grid=np.sort(np.concatenate((xl,xr)))\n",
    "prod=list(itertools.product(axis_grid, repeat=p))\n",
    "grid=np.array(prod)\n",
    "grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "258d605a-9628-485e-b199-96856f752592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation progress for error distribution norm with n_samples = 2000 n_trees =  50  and sigma = 0.5 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [20:01<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation for error distribution norm with sigma = 0.5  and n_trees = 50  complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# parameters to vary:\n",
    "\n",
    "# number of confidence bands\n",
    "n_tests=1000\n",
    "\n",
    "#random seed for both data and tree construction (two generators for replicable results)\n",
    "rand_seed=0\n",
    "\n",
    "# vector of sample sizes\n",
    "sample_sizes=[2000] #[250,500,1000,2000,4000,8000]\n",
    "\n",
    "# vector of error std, used: [0.75,1,1.25]\n",
    "sigs=[0.5]#[0.75,1,1.25]\n",
    "\n",
    "#vector of error distributions, possible entries: 'norm', 'uni, 'tdx' where x is a place holder for the degrees of freedom\n",
    "distributions=['norm']#'uni','td4','td6']#'norm', \n",
    "\n",
    "# vector with number of trees\n",
    "n_trees_vec=[50]#,100] \n",
    "\n",
    "#multiplier to in/exclude the regression function\n",
    "# 1 for the normal regression model, 0 to set m=0 to test the asymptotic distribution without approximation error\n",
    "m_factor=1 \n",
    "\n",
    "#_____________________________________________________________________________________________________\n",
    "\n",
    "X_test=grid\n",
    "m_true_grid=m_factor*regression_fct(X_test)\n",
    "\n",
    "#Simulation run\n",
    "for sigma in sigs:\n",
    "    for dist_name in distributions:\n",
    "        if dist_name == \"norm\":\n",
    "            error_dist=stats.norm(loc=0, scale=sigma)\n",
    "        elif dist_name == \"uni\":\n",
    "            error_dist=stats.uniform(loc=-0.5*sigma*np.sqrt(12), scale=sigma*np.sqrt(12))\n",
    "        elif dist_name[:2]==\"td\":\n",
    "            try:\n",
    "                df=int(dist_name[2:len(dist_name)])\n",
    "            except Exception as e:\n",
    "                print(f\"An error occured: {e}.\", \"Distribution not known. Skipped to next distribution.\")\n",
    "                continue\n",
    "            error_dist=stats.t(df=df,loc=0,scale = sigma* np.sqrt((df-2)/df))\n",
    "        else:\n",
    "            print(\"Distribution not known. Skipped to next distribution.\")\n",
    "            continue\n",
    "        \n",
    "        for n_trees in n_trees_vec:\n",
    "\n",
    "            for n_samples in sample_sizes:\n",
    "                   \n",
    "                # asymptotic standard deviations (pointwise)\n",
    "                as_std_uni=np.sqrt(2**(2*k)*V_cap_uni/n_samples)\n",
    "                as_std_ehr=np.sqrt(2**(2*k)*V_cap_ehr/n_samples)\n",
    "                \n",
    "                #confidence band radii based on the quantiles and the standard deviations\n",
    "                cb_rad_uni=as_std_uni*quants_uni\n",
    "                cb_rad_ehr=as_std_ehr*quants_ehr\n",
    "                    \n",
    "                #bandwidth for the variance estimator\n",
    "                h_g=1/n_samples**(1/2) \n",
    "\n",
    "                sups_uni=[]\n",
    "                sups_ehr=[]\n",
    "                sigma_hats=[] \n",
    "    \n",
    "                print(\"Simulation progress for error distribution \"+dist_name+\" with n_samples =\",n_samples,\"n_trees = \",n_trees,\" and sigma =\",sigma, \":\")\n",
    "                \n",
    "                np.random.seed(rand_seed) \n",
    "                data_rng = np.random.default_rng(rand_seed)\n",
    "                \n",
    "                model_uni=RandomForestModel(n_trees=n_trees, max_depth=k,sample_size_fct=r,tree_type=\"Uni\")\n",
    "                model_ehr=RandomForestModel(n_trees=n_trees, max_depth=k,sample_size_fct=r,tree_type=\"Ehr\",delta=delta,B=B)\n",
    "                \n",
    "                for i in tqdm(range(n_tests)):\n",
    "                    #data from a different random generator than the one used for the random forests\n",
    "                    e = error_dist.rvs(size=n_samples,random_state=data_rng)\n",
    "                    X = data_rng.random(n_samples*p).reshape(n_samples,p)\n",
    "                    \n",
    "                    m = m_factor*regression_fct(X)\n",
    "                    Y=m+e\n",
    "                    \n",
    "                    model_uni.clear()\n",
    "                    model_uni.train(X,Y)\n",
    "                    model_ehr.clear()\n",
    "                    model_ehr.train(X,Y)\n",
    "                    \n",
    "                    preds_uni=model_uni.predict(X_test)\n",
    "                    preds_ehr=model_ehr.predict(X_test)\n",
    "                    \n",
    "                    error_uni=preds_uni-m_true_grid\n",
    "                    error_ehr=preds_ehr-m_true_grid\n",
    "                    \n",
    "                    sup_uni=np.max(np.abs(error_uni))\n",
    "                    sup_ehr=np.max(np.abs(error_ehr))\n",
    "                    \n",
    "                    sigma_hat=np.sqrt(fcts.sigma_hat_gauss(X,Y,h_g))\n",
    "                    \n",
    "                    sups_uni.append(sup_uni)\n",
    "                    sups_ehr.append(sup_ehr)\n",
    "                    \n",
    "                    sigma_hats.append(sigma_hat)\n",
    "                    \n",
    "                cover_num_uni=np.zeros(3)\n",
    "                cover_num_ehr=np.zeros(3)\n",
    "                for j in range(len(cb_rad_uni)):\n",
    "                    cover_num_uni[j]=sum(np.array(sups_uni)<np.array(sigma_hats)*cb_rad_uni[j])\n",
    "                    cover_num_ehr[j]=sum(np.array(sups_ehr)<np.array(sigma_hats)*cb_rad_ehr[j])\n",
    "                n_CB=len(sups_uni)\n",
    "                \n",
    "                avg_cb_rad_uni=cb_rad_uni*np.mean(sigma_hats)\n",
    "                avg_cb_rad_ehr=cb_rad_ehr*np.mean(sigma_hats)\n",
    "    \n",
    "                if m_factor==1:\n",
    "                    m_info = \" and m = \"+regression_fct.__name__\n",
    "                elif m_factor ==0:\n",
    "                    m_info = \" and m set to zero.\"\n",
    "                else:\n",
    "                    m_info = \" and m = \"+str(m_factor)+\"*\"+regression_fct.__name__\n",
    "                \n",
    "                result_txt = [\"\",\"\",\"Results  at \"+time.ctime(),\n",
    "                              \"for Random Seed = \"+str(rand_seed),\n",
    "                              \"and Regression model with:\",\n",
    "                              \"p = \"+str(p)+m_info,\n",
    "                              \"Parameters:\",\n",
    "                              \"Sample size: \"+str(n_samples),\n",
    "                              \"k=\"+str(k),\n",
    "                              \"r=\"+str(r*n_samples)]\n",
    "                if error_dist.dist.name == 't':\n",
    "                    result_txt.append(\"Error distribution: t-Distribution with \"+str(df)+\" degrees of freedom\")\n",
    "                else:\n",
    "                    result_txt.append(\"Error distribution: \" +str(error_dist.dist.name))\n",
    "                \n",
    "                result_txt.append(\"Error std: \"+str(sigma))\n",
    "                result_txt.append(\"Number of trees: \"+str(n_trees))\n",
    "                result_txt.append(\"Number of CBS: \"+str(n_CB))\n",
    "                result_txt+=[\"\",\"Empirical Coverage for confidence bands with theoretical coverage \"+ str(1-beta)+\":\"]\n",
    "                result_txt.append(\"Uniform CPRF, number: \"+str(cover_num_uni)+\", percentage: \"+ str(cover_num_uni/n_CB))\n",
    "                result_txt.append(\"Ehrenfest CPRF, number: \"+str(cover_num_ehr)+\", percentage: \"+ str(cover_num_ehr/n_CB))\n",
    "                result_txt+=[\"\",\"Average confidence band radius for theoretical coverage \"+ str(1-beta)+\":\"]\n",
    "                result_txt.append(\"Uniform CPRF: \"+str(avg_cb_rad_uni))\n",
    "                result_txt.append(\"Ehrenfest CPRF: \"+str(avg_cb_rad_ehr))\n",
    "                \n",
    "                # save results of combination in txt file\n",
    "                with open('Simulation results.txt', 'a') as f:\n",
    "                    for line in result_txt:\n",
    "                        f.write(line + '\\n')\n",
    "                \n",
    "                print(\"Simulation for error distribution \"+dist_name+\" with sigma =\",sigma,\" and n_trees =\",n_trees, \" complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60579b1a-26cd-4dc2-a3f3-130790ed9f38",
   "metadata": {},
   "source": [
    "# Histogram\n",
    "This section is for the simulation of asymptotic confidence bands for the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f9435055-b43c-493b-a123-1dd44e437ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram parameter: n_h is ne number of intervals that each axis is divided into\n",
    "# we used n_h=5,7\n",
    "n_h = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5a0fe0-9933-4e8d-8664-682255706b2a",
   "metadata": {},
   "source": [
    "## Quantile estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c497c1dd-6487-461f-9fbc-8314c850d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimation of the supremum distribution for the histogram estimator\n",
    "n_sups=100000\n",
    "np.random.seed(0)\n",
    "sups_histo=[]\n",
    "for _ in range(n_sups):\n",
    "    e=np.random.normal(0,1,n_h**p)\n",
    "    sups_histo.append(np.max(np.abs(e)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5fc6a35a-61cb-4018-8c4a-7a0af50af7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantiles at:    [0.9  0.95 0.99]\n",
      "Histogram:       [3.06892471 3.2777789  3.70602254]\n"
     ]
    }
   ],
   "source": [
    "#empirical quantiles\n",
    "quants_histo=np.quantile(sups_histo,1-beta)\n",
    "\n",
    "#stats.describe(sups_GP_uni), stats.describe(sups_GP_ehr)\n",
    "print(\"Quantiles at:   \",1-beta)\n",
    "print(\"Histogram:      \", quants_histo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ffe519d-24d8-4369-961a-0acd764858ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence band radii for n =  2000  and n_h =  7 with confidence levels : [0.9  0.95 0.99]\n",
      "Histogram:       [0.4803627  0.51305355 0.58008428]\n"
     ]
    }
   ],
   "source": [
    "# training sample size\n",
    "n_samples = 2000 #4000\n",
    "\n",
    "#factor for subsample size\n",
    "r = 0.75  \n",
    "\n",
    "# bandwidth for estimation of sigma by kernel estimator\n",
    "h_g=1/n_samples**(1/2)\n",
    "\n",
    "# asymptotic standard deviations (pointwise)\n",
    "as_std_histo=np.sqrt(n_h**p/n_samples)\n",
    "\n",
    "#confidence band radii based on the quantiles and the standard deviations\n",
    "cb_rad_histo=as_std_histo*quants_histo\n",
    "\n",
    "print(\"Confidence band radii for n = \",n_samples,\" and n_h = \",n_h, \"with confidence levels :\", 1-beta)\n",
    "print(\"Histogram:      \", cb_rad_histo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "89696992-6d23-4cfe-a915-877caf4133c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(441, 2)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid for supremum calculation histogram\n",
    "# estimation points at center and corners of cells\n",
    "eps=1/2**20\n",
    "g=n_h\n",
    "splits=np.linspace(0, 1, num=g, endpoint=False)\n",
    "xl=splits+eps\n",
    "xr=splits+1/g-eps\n",
    "xm=splits+1/(2*n_h)\n",
    "axis_grid=np.sort(np.concatenate((xl,xr,xm)))\n",
    "prod=list(itertools.product(axis_grid, repeat=p))\n",
    "histo_grid=np.array(prod)\n",
    "histo_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "13d26edc-1bc8-4971-bbe8-bfcf87762755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation progress for error distribution norm with n_h =  7 , n_samples = 250  and sigma = 1 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:05<00:00, 183.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation for error distribution norm with n_h =  7 , n_samples = 250  and sigma = 1  complete.\n",
      "Simulation progress for error distribution norm with n_h =  7 , n_samples = 500  and sigma = 1 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:25<00:00, 39.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation for error distribution norm with n_h =  7 , n_samples = 500  and sigma = 1  complete.\n",
      "Simulation progress for error distribution norm with n_h =  7 , n_samples = 1000  and sigma = 1 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:39<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation for error distribution norm with n_h =  7 , n_samples = 1000  and sigma = 1  complete.\n",
      "Simulation progress for error distribution norm with n_h =  7 , n_samples = 2000  and sigma = 1 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [07:24<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation for error distribution norm with n_h =  7 , n_samples = 2000  and sigma = 1  complete.\n",
      "Simulation progress for error distribution norm with n_h =  7 , n_samples = 4000  and sigma = 1 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [34:29<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation for error distribution norm with n_h =  7 , n_samples = 4000  and sigma = 1  complete.\n",
      "Simulation progress for error distribution norm with n_h =  7 , n_samples = 8000  and sigma = 1 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [2:31:04<00:00,  9.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation for error distribution norm with n_h =  7 , n_samples = 8000  and sigma = 1  complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# parameters to vary:\n",
    "\n",
    "# number of confidence bands\n",
    "n_tests=1000\n",
    "\n",
    "# random seed for both data and tree construction (two generators for replicable results)\n",
    "rand_seed=0\n",
    "\n",
    "# vector of sample sizes\n",
    "sample_sizes=[250,500,1000,2000,4000,8000]\n",
    "\n",
    "# vector of error std, used: [0.75,1,1.25]\n",
    "sigs=[1]#[0.75,1,1.25]\n",
    "\n",
    "# vector of error distributions, possible entries: 'norm', 'uni, 'tdx' where x is a place holder for the degrees of freedom\n",
    "distributions=['norm']#,'uni','td4','td6'] \n",
    "\n",
    "# multiplier to in/exclude the regression function\n",
    "# 1 for the normal regression model, 0 to set m=0 to test the asymptotic distribution without approximation error\n",
    "m_factor=1 \n",
    "\n",
    "#_____________________________________________________________________________________________________\n",
    "\n",
    "X_test=histo_grid \n",
    "m_true_grid=m_factor*regression_fct(X_test)\n",
    "\n",
    "for sigma in sigs:\n",
    "    for dist_name in distributions:\n",
    "        if dist_name == \"norm\":\n",
    "            error_dist=stats.norm(loc=0, scale=sigma)\n",
    "        elif dist_name == \"uni\":\n",
    "            error_dist=stats.uniform(loc=-0.5*sigma*np.sqrt(12), scale=sigma*np.sqrt(12))\n",
    "        elif dist_name[:2]==\"td\":\n",
    "            try:\n",
    "                df=int(dist_name[2:len(dist_name)])\n",
    "            except Exception as e:\n",
    "                print(f\"An error occured: {e}.\", \"Distribution not known. Skipped to next distribution.\")\n",
    "                continue\n",
    "            error_dist=stats.t(df=df,loc=0,scale = sigma* np.sqrt((df-2)/df))\n",
    "        else:\n",
    "            print(\"Distribution not known. Skipped to next distribution.\")\n",
    "            continue\n",
    "           \n",
    "        for n_samples in sample_sizes:\n",
    "            sups_histo=[]\n",
    "            sigma_hats=[]  \n",
    "\n",
    "            #asymptotic standard deviation (pointwise)\n",
    "            as_std_histo=np.sqrt(n_h**p/n_samples)\n",
    "            \n",
    "            #confidence band radii based on the quantiles and the standard deviations\n",
    "            cb_rad_histo=as_std_histo*quants_histo\n",
    "\n",
    "            #bandwidth for the variance estimator\n",
    "            h_g=1/n_samples**(1/2) \n",
    "\n",
    "            print(\"Simulation progress for error distribution \"+dist_name+\" with n_h = \", n_h, \", n_samples =\",n_samples,\" and sigma =\",sigma, \":\")\n",
    "  \n",
    "            np.random.seed(rand_seed)\n",
    "            data_rng = np.random.default_rng(rand_seed) \n",
    "            \n",
    "            model_histo=HistogramEstimator(n_cells=n_h)\n",
    "            \n",
    "            for i in tqdm(range(n_tests)):\n",
    "                #data from a different random generator than the one used for the random forests\n",
    "                e = error_dist.rvs(size=n_samples,random_state=data_rng)\n",
    "                X = data_rng.random(n_samples*p).reshape(n_samples,p)\n",
    "                \n",
    "                m = m_factor*regression_fct(X)\n",
    "                Y=m+e\n",
    "                \n",
    "                model_histo.clear()\n",
    "                model_histo.train(X,Y)        \n",
    "                \n",
    "                preds_histo=model_histo.predict(X_test)\n",
    "                \n",
    "                error_histo=preds_histo - m_true_grid\n",
    "                \n",
    "                sup_histo=np.max(np.abs(error_histo))\n",
    "                \n",
    "                sigma_hat=np.sqrt(fcts.sigma_hat_gauss(X,Y,h_g))\n",
    "                \n",
    "                sups_histo.append(sup_histo)\n",
    "                \n",
    "                sigma_hats.append(sigma_hat)\n",
    "                \n",
    "            cover_num_histo=np.zeros(3)\n",
    "            for j in range(len(cb_rad_histo)):\n",
    "                cover_num_histo[j]=sum(np.array(sups_histo)<np.array(sigma_hats)*cb_rad_histo[j])\n",
    "            n_CB=len(sups_histo)\n",
    "            \n",
    "            avg_cb_rad_histo=cb_rad_histo*np.mean(sigma_hats)\n",
    "    \n",
    "            #_________________________________________________________________________\n",
    "            if m_factor==1:\n",
    "                m_info = \" and m = \"+regression_fct.__name__\n",
    "            elif m_factor ==0:\n",
    "                m_info = \" and m set to zero.\"\n",
    "            else:\n",
    "                m_info = \" and m = \"+str(m_factor)+\"*\"+regression_fct.__name__\n",
    "            \n",
    "            result_txt = [\"\",\"\",\"Results  at \"+time.ctime(),\n",
    "                          \"for Random Seed = \"+str(rand_seed),\n",
    "                          \"and Regression model with:\",\n",
    "                          \"p = \"+str(p)+m_info,\n",
    "                          \"Parameters:\",\n",
    "                          \"Sample size: \"+str(n_samples),\n",
    "                          \"n_h=\"+str(n_h)]\n",
    "            \n",
    "            if error_dist.dist.name == 't':\n",
    "                result_txt.append(\"Error distribution: t-Distribution with \"+str(df)+\" degrees of freedom\")\n",
    "            else:\n",
    "                result_txt.append(\"Error distribution: \" +str(error_dist.dist.name))\n",
    "            \n",
    "            result_txt.append(\"Error std: \"+str(sigma))\n",
    "            result_txt.append(\"Number of CBS: \"+str(n_CB))\n",
    "            result_txt+=[\"\",\"Empirical Coverage for confidence bands with theoretical coverage \"+ str(1-beta)+\":\"]\n",
    "            result_txt.append(\"Histogram, number: \"+str(cover_num_histo)+\", percentage: \"+ str(cover_num_histo/n_CB))\n",
    "            result_txt+=[\"\",\"Average confidence band radius for theoretical coverage \"+ str(1-beta)+\":\"]\n",
    "            result_txt.append(\"Histogram: \"+str(avg_cb_rad_histo))\n",
    "            \n",
    "            # save results of combination in txt file\n",
    "            with open('Simulation results.txt', 'a') as f:\n",
    "                for line in result_txt:\n",
    "                    f.write(line + '\\n')\n",
    "            \n",
    "            print(\"Simulation for error distribution \"+dist_name+\" with n_h = \", n_h, \", n_samples =\",n_samples,\" and sigma =\",sigma, \" complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4b75bb6-8406-47e2-8a6c-3ac45cf45dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'Version control.txt' was created.\n"
     ]
    }
   ],
   "source": [
    "# if necessary create a text file for the version control\n",
    "import pkg_resources\n",
    "import sys\n",
    "\n",
    "if not os.path.isfile('Version control.txt'):\n",
    "    with open('Version control.txt', 'w') as f:\n",
    "        f.write( f\"Python Version: {sys.version}\\n\")\n",
    "        installed_packages = pkg_resources.working_set\n",
    "        packages_sorted = sorted(f\"{p.project_name}=={p.version}\" for p in installed_packages)\n",
    "        f.write(\"\\n\".join(packages_sorted))        \n",
    "    print(\"The file 'Version control.txt' was created.\")\n",
    "else:\n",
    "    print(\"The file 'Version control.txt' already existed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
