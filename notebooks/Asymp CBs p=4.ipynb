{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1639b0a-e7fa-48ae-ac6c-78019a0da82d",
   "metadata": {},
   "source": [
    "# Notebook for Testing different confidence bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55764d12-48a7-4c34-91a4-cd96317f14c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from IPython.display import display, HTML\n",
    "from scipy import stats\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import special\n",
    "from numpy.linalg import cholesky\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse.linalg import splu\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), \"../src\"))\n",
    "sys.path.append(src_path)\n",
    "from forest_v2 import RegressionTreeModel\n",
    "from forest_v2 import RandomForestModel\n",
    "from forest_v2 import HistogramEstimator\n",
    "import functions as fcts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea608290-e388-4596-9fe0-5f23c608b65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension of the feature space\n",
    "p = 4\n",
    "\n",
    "#regression function\n",
    "regression_fct=fcts.m_p4_01\n",
    "\n",
    "# betas for confidence levels 1-\\beta\n",
    "beta=np.array([0.1,0.05,0.01])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d44ed-ba61-4ca1-b86b-157ed5b97b7a",
   "metadata": {},
   "source": [
    "## Covariance estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caa1489b-ff79-4fac-b9ba-ae54f0ff29b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth of the trees\n",
    "k = 5 #5\n",
    "\n",
    "# smaller k for evaluation grid (choice justified later)\n",
    "k2 = 3 #3\n",
    "\n",
    "#Ehrenfest parameters\n",
    "B = 12\n",
    "delta = 7\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "261b2d88-e3fb-4420-a586-a90867dbc110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform V_cap: 0.00907939453125 , Ehrenfest V_cap: 0.009599375 , 2^-k: 0.03125 , 2^-2k: 0.0009765625\n"
     ]
    }
   ],
   "source": [
    "# estimation of all possible covariance entries using the parameters for the partition construction\n",
    "np.random.seed(0)\n",
    "n_parts=50000 # number of split pairs generated\n",
    "\n",
    "# list of all combinations of closeness relations between two points in the feature space\n",
    "comb_list=[]\n",
    "for com in itertools.combinations_with_replacement(range(0, k + 1), p):\n",
    "    comb_list.append(com)\n",
    "combs=np.array(comb_list)\n",
    "\n",
    "#simulation of splits according to the uniform CPRF and calculation of the average intersection volume for all combinations\n",
    "cu_vol=np.zeros(len(combs))\n",
    "for j in range(n_parts):\n",
    "    s1=np.bincount(np.random.choice(p, k, replace=True),minlength=p)\n",
    "    s2=np.bincount(np.random.choice(p, k, replace=True),minlength=p)\n",
    "    for i in range(len(combs)):\n",
    "        cu_vol[i]+=fcts.vol_intersec_2(combs[i],s1,s2)\n",
    "av_vol=cu_vol/n_parts\n",
    "\n",
    "# calculation of estimated covariance from average volumina\n",
    "V_cap_uni=av_vol[len(av_vol)-1]\n",
    "cov_entries_uni=av_vol/V_cap_uni\n",
    "\n",
    "#simulation of splits according to the Ehrenfest CPRF and calculation of the average intersection volume for all combinations\n",
    "cu_vol=np.zeros(len(combs))\n",
    "for j in range(n_parts):\n",
    "    s1=fcts.ehr_splits(np.zeros(p),np.ones(p)*B,delta,k)\n",
    "    s2=fcts.ehr_splits(np.zeros(p),np.ones(p)*B,delta,k)\n",
    "    for i in range(len(combs)):\n",
    "        cu_vol[i]+=fcts.vol_intersec_2(combs[i],s1,s2)\n",
    "av_vol=cu_vol/n_parts\n",
    "\n",
    "# calculation of estimated covariance from average volumina\n",
    "V_cap_ehr=av_vol[len(av_vol)-1]\n",
    "cov_entries_ehr=av_vol/V_cap_ehr\n",
    "\n",
    "print(\"Uniform V_cap:\", V_cap_uni,\", Ehrenfest V_cap:\", V_cap_ehr,\", 2^-k:\", 1/2**k,\", 2^-2k:\",1/2**(2*k))\n",
    "\n",
    "#cov_entries_ehr, cov_entries_uni, #len(cov_entries_uni),special.binom(k+p,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22a9c2c-68a4-46e2-8ba1-2759312cb4fe",
   "metadata": {},
   "source": [
    "Justification for the choice of $k_2$. For any pair of points in the feature space with closeness $(k_2,k_2)$ or larger, the Gaussian process' covariance should be close to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d537cb52-70f1-4680-901f-51f38be986d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest closeness with cov > min_prob, Ehr.: (3, 3, 3, 3) and Uni: (3, 3, 3, 3)\n",
      "Corresponding covariances:  0.9989582655120777 and 0.9980725586998376\n"
     ]
    }
   ],
   "source": [
    "min_prob=0.99 # 0.99\n",
    "print(\"Smallest closeness with cov > min_prob, Ehr.:\",comb_list[np.argmax(cov_entries_ehr>min_prob)], \"and Uni:\", comb_list[np.argmax(cov_entries_uni>min_prob)])\n",
    "print(\"Corresponding covariances: \", cov_entries_ehr[np.argmax(cov_entries_ehr>min_prob)], \"and\",cov_entries_uni[np.argmax(cov_entries_uni>min_prob)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c869038a-0e70-4333-9793-fe373b5ebf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laufzeit: 25.83875870704651 Sekunden\n",
      "Size of the cov matrix:  (4096, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Filling in the covariance entries in the matrices\n",
    "X_test=fcts.test_grid(p,k2)\n",
    "start_time = time.time()\n",
    "X_test_M=np.array(list(itertools.product(X_test, repeat=2)))#.reshape((2**(k2*p),2**(k2*p),2,2))\n",
    "X_test_V=X_test_M.reshape(2**(k2*p*2),2*p)\n",
    "t_values=np.arange(0,k+1)\n",
    "v_t=(2**t_values).reshape(-1,1)\n",
    "fin_com_cell=np.empty((2**(k2*p*2),p))\n",
    "for i in range(p):\n",
    "    X_temp_1=X_test_V[:,i].reshape(-1,1)\n",
    "    X_temp_2=X_test_V[:,i+p].reshape(-1,1)\n",
    "    ints1=np.dot(X_temp_1,v_t.T).astype(int)\n",
    "    ints2=np.dot(X_temp_2,v_t.T).astype(int)\n",
    "    equal_mask=ints1==ints2\n",
    "    fin_com_cell[:,i] = np.max(np.where(equal_mask, t_values, -np.inf),axis=1)\n",
    "M=fin_com_cell.astype(int)\n",
    "M=np.sort(M,axis=1)\n",
    "M_int=np.sum(M*10**np.arange(p-1,-1,-1),axis=1)\n",
    "int_combs=np.sum(combs*10**np.arange(p-1,-1,-1),axis=1)\n",
    "indices = np.searchsorted(int_combs, M_int)\n",
    "cov_uni=cov_entries_uni[indices].reshape((2**(k2*p),2**(k2*p)))\n",
    "cov_ehr=cov_entries_ehr[indices].reshape((2**(k2*p),2**(k2*p)))\n",
    "end_time = time.time()\n",
    "print(f\"Laufzeit: {end_time - start_time} Sekunden\")\n",
    "print(\"Size of the cov matrix: \",cov_uni.shape) #, np.info(cov_uni), np.info(cov_ehr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174b9a7b-e94f-4228-a919-f8798b721afc",
   "metadata": {},
   "source": [
    "The next two cells perform the Cholesky decomposition of the covariance matrices. If this is not possible due to the estimation error, they reconstruct the matrix beforehand. Either based on all eigenvalues larger than $\\epsilon$ and their corresponding eigenvectors or based on the SVD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fc08bb0-c784-4859-8355-1754792eb321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occured: Matrix is not positive definite. We used reconstruction of the covariance matrix via eigenvectors before the cholesky decomposition.\n"
     ]
    }
   ],
   "source": [
    "epsilon=1e-6\n",
    "try:\n",
    "    L_uni = cholesky(cov_uni+epsilon * np.eye(2**(k2*p))) #+1e-3 * np.eye(2**(k*p))\n",
    "except Exception as e:\n",
    "    eig_values, eig_vectors = np.linalg.eigh(cov_uni)\n",
    "    corrected_eig_values = np.maximum(eig_values, epsilon)\n",
    "    fixed_cov_uni = eig_vectors @ np.diag(corrected_eig_values) @ eig_vectors.T\n",
    "    L_uni = cholesky(fixed_cov_uni)\n",
    "    print(f\"An error occured: {e}.\", \"We used reconstruction of the covariance matrix via eigenvectors before the cholesky decomposition.\")\n",
    "else:\n",
    "    print(\"We used the cholesky decomposition of the original covariance matrix.\")  \n",
    "#stats.describe((L_uni @ L_uni.T)[:,0] -cov_uni[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80412931-f77c-4b62-b918-d8749ac18155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occured: Matrix is not positive definite. We used reconstruction of the covariance matrix via eigenvectors before the cholesky decomposition.\n"
     ]
    }
   ],
   "source": [
    "epsilon=1e-6\n",
    "try:\n",
    "    L_ehr = cholesky(cov_ehr+epsilon * np.eye(2**(k2*p))) #+1e-3 * np.eye(2**(k*p))\n",
    "except Exception as e:\n",
    "    eig_values, eig_vectors = np.linalg.eigh(cov_ehr)\n",
    "    corrected_eig_values = np.maximum(eig_values, epsilon)\n",
    "    fixed_cov_ehr = eig_vectors @ np.diag(corrected_eig_values) @ eig_vectors.T\n",
    "    L_ehr = cholesky(fixed_cov_ehr)\n",
    "    print(f\"An error occured: {e}.\", \"We used reconstruction of the covariance matrix via eigenvectors before the cholesky decomposition.\")\n",
    "else:\n",
    "    print(\"We used the cholesky decomposition of the original covariance matrix.\")\n",
    "#stats.describe((L_ehr @ L_ehr.T)[:,0] -cov_ehr[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5657a42d-cfa4-481f-b695-8fa54e061a30",
   "metadata": {},
   "source": [
    "## Quantile estimation of the Gaussian processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e68126c-5799-4e8b-860e-ca1d17de873e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 100000/100000 [23:24<00:00, 71.21it/s]\n"
     ]
    }
   ],
   "source": [
    "#Generating suprema of the Gaussian process to estimate the quantiles\n",
    "np.random.seed(0)\n",
    "sups_GP_uni=[]\n",
    "sups_GP_ehr=[]\n",
    "# number of suprema\n",
    "n_sups=100000\n",
    "for _ in tqdm(range(n_sups)):\n",
    "    e=np.random.normal(0,1,2**(k2*p))\n",
    "    gp_uni = L_uni @ e\n",
    "    gp_ehr = L_ehr @ e\n",
    "    sups_GP_uni.append(np.max(np.abs(gp_uni)))\n",
    "    sups_GP_ehr.append(np.max(np.abs(gp_ehr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ccd3fbd6-bd45-4d9b-ae47-4710d1db22b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantiles for k =  5  at:    [0.9  0.95 0.99]\n",
      "Uniform CPRF:    [3.65631766 3.86412559 4.29084145]\n",
      "Ehrenfest CPRF:  [3.61200856 3.82227777 4.25334828]\n"
     ]
    }
   ],
   "source": [
    "#empirical quantiles\n",
    "quants_uni=np.quantile(sups_GP_uni,1-beta)\n",
    "quants_ehr=np.quantile(sups_GP_ehr,1-beta)\n",
    "\n",
    "#stats.describe(sups_GP_uni), stats.describe(sups_GP_ehr)\n",
    "print(\"Quantiles for k = \",k,\" at:   \",1-beta)\n",
    "print(\"Uniform CPRF:   \",quants_uni)\n",
    "print(\"Ehrenfest CPRF: \", quants_ehr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49980670-3a36-47cf-9191-ad21f482caa7",
   "metadata": {},
   "source": [
    "# CB Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3274d2f7-41ec-4b6d-958a-00f6d2478033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'Simulation results.txt' already existed.\n"
     ]
    }
   ],
   "source": [
    "# if necessary create a text file for the results of the simulations\n",
    "if not os.path.isfile('Simulation results.txt'):\n",
    "    with open('Simulation results.txt', 'w') as f:\n",
    "        f.write( \"Simulation results of asymptotic confidence bands\"+ '\\n')\n",
    "    print(\"The file 'Simulation results.txt' was created.\")\n",
    "else:\n",
    "    print(\"The file 'Simulation results.txt' already existed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5c109e-2421-494d-a29d-b7b26d0d33cf",
   "metadata": {},
   "source": [
    "Set the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce8a001b-7a1f-4923-bf33-3bbaf2128c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training sample size\n",
    "n_samples = 2000 #4000\n",
    "\n",
    "#factor for subsample size\n",
    "r = 0.75 \n",
    "\n",
    "# bandwidth for estimation of sigma by kernel estimator\n",
    "h_g=1/n_samples**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28defa1f-c049-46f1-b368-74f0495c72e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence band radii for n =  2000  p =  4  and k =  5 with confidence levels : [0.9  0.95 0.99]\n",
      "Uniform CPRF:    [0.24929143 0.26345998 0.2925539 ]\n",
      "Ehrenfest CPRF:  [0.25322421 0.26796539 0.29818611]\n"
     ]
    }
   ],
   "source": [
    "# asymptotic standard deviations (pointwise)\n",
    "as_std_uni=np.sqrt(2**(2*k)*V_cap_uni/n_samples)\n",
    "as_std_ehr=np.sqrt(2**(2*k)*V_cap_ehr/n_samples)\n",
    "\n",
    "#confidence band radii based on the quantiles and the standard deviations\n",
    "cb_rad_uni=as_std_uni*quants_uni\n",
    "cb_rad_ehr=as_std_ehr*quants_ehr\n",
    "\n",
    "print(\"Confidence band radii for n = \",n_samples,\" p = \", p, \" and k = \",k, \"with confidence levels :\", 1-beta)\n",
    "print(\"Uniform CPRF:   \",cb_rad_uni)\n",
    "print(\"Ehrenfest CPRF: \", cb_rad_ehr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09845ef8-8ee6-467b-bbcd-4a2fe0a18caf",
   "metadata": {},
   "source": [
    "The next cell is for the simulation of asymptotic confidence bands for the two random forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65ddb2dc-0968-4a91-ae25-4de32845d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(i, n_samples, p, m_factor, regression_fct, delta, B, error_dist, rand_seed, n_trees, k, r, X_test, m_true_grid, h_g):\n",
    "    np.random.seed(rand_seed + i)  # Use a unique seed for each iteration\n",
    "    data_rng = np.random.default_rng(rand_seed + i)  # Create a new random number generator for each iteration\n",
    "\n",
    "    # Generate data\n",
    "    e = error_dist.rvs(size=n_samples, random_state=data_rng)\n",
    "    X = data_rng.random(size=(n_samples , p))\n",
    "\n",
    "    m = m_factor * regression_fct(X)\n",
    "    Y = m + e\n",
    "\n",
    "    # Instantiate models\n",
    "    model_uni = RandomForestModel(n_trees=n_trees, max_depth=k, sample_size_fct=r, tree_type=\"Uni\")\n",
    "    model_ehr = RandomForestModel(n_trees=n_trees, max_depth=k, sample_size_fct=r, tree_type=\"Ehr\", delta=delta, B=B)\n",
    "\n",
    "    # Train models\n",
    "    model_uni.train(X, Y)\n",
    "    model_ehr.train(X, Y)\n",
    "\n",
    "    # Make predictions\n",
    "    preds_uni = model_uni.predict(X_test)\n",
    "    preds_ehr = model_ehr.predict(X_test)\n",
    "\n",
    "    # Calculate errors\n",
    "    error_uni = preds_uni - m_true_grid\n",
    "    error_ehr = preds_ehr - m_true_grid\n",
    "\n",
    "    sup_uni = np.max(np.abs(error_uni))\n",
    "    sup_ehr = np.max(np.abs(error_ehr))\n",
    "\n",
    "    sigma_hat = np.sqrt(fcts.sigma_hat_gauss(X, Y, h_g))\n",
    "\n",
    "    return sup_uni, sup_ehr, sigma_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e96efeda-1d0b-4e9d-87a6-30be0a0e5dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65536, 4)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grid for supremum calculation\n",
    "eps=1/2**20\n",
    "g=2**k2\n",
    "splits=np.linspace(0, 1, num=g, endpoint=False)\n",
    "xl=splits+eps\n",
    "xr=splits+1/g-eps\n",
    "axis_grid=np.sort(np.concatenate((xl,xr)))\n",
    "prod=list(itertools.product(axis_grid, repeat=p))\n",
    "grid=np.array(prod)\n",
    "grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "258d605a-9628-485e-b199-96856f752592",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation progress for error distribution norm with sigma = 1  and n_trees = 100 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 1000/1000 [17:07:22<00:00, 61.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation for distribution norm with sigma = 1  and n_trees = 100  complete.\n",
      "Simulation progress for error distribution norm with sigma = 1  and n_trees = 50 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [8:33:10<00:00, 30.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation for distribution norm with sigma = 1  and n_trees = 50  complete.\n"
     ]
    }
   ],
   "source": [
    "# parameters to vary:\n",
    "\n",
    "# number of cpus used\n",
    "n_cpus=2\n",
    "\n",
    "# number of confidence bands\n",
    "n_tests=1000\n",
    "\n",
    "# random seed for both data and tree construction (two generators for replicable results)\n",
    "rand_seed=12345\n",
    "\n",
    "# vector of error std, used: [0.75,1,1.25]\n",
    "sigs=[1]#0.5,0.75,1,1.25]\n",
    "\n",
    "# vector of error distributions, possible entries: 'norm', 'uni, 'tdx' where x is a place holder for the degrees of freedom\n",
    "distributions=['norm']#,'uni','td4','td6'] \n",
    "\n",
    "# vector with number of trees\n",
    "n_trees_vec=[100,50]#,100] \n",
    "\n",
    "# multiplier to in/exclude the regression function\n",
    "# 1 for the normal regression model, 0 to set m=0 to test the asymptotic distribution without approximation error\n",
    "m_factor=0\n",
    "\n",
    "#bandwidth for the variance estimator\n",
    "h_g=1/n_samples**(1/2) \n",
    "\n",
    "#_____________________________________________________________________________________________________\n",
    "\n",
    "X_test=grid \n",
    "m_true_grid=m_factor*regression_fct(X_test)\n",
    "\n",
    "#Simulation run\n",
    "for sigma in sigs:\n",
    "    for dist_name in distributions:\n",
    "        if dist_name == \"norm\":\n",
    "            error_dist=stats.norm(loc=0, scale=sigma)\n",
    "        elif dist_name == \"uni\":\n",
    "            error_dist=stats.uniform(loc=-0.5*sigma*np.sqrt(12), scale=sigma*np.sqrt(12))\n",
    "        elif dist_name[:2]==\"td\":\n",
    "            try:\n",
    "                df=int(dist_name[2:len(dist_name)])\n",
    "            except Exception as e:\n",
    "                print(f\"An error occured: {e}.\", \"Distribution not known. Skipped to next distribution.\")\n",
    "                continue\n",
    "            error_dist=stats.t(df=df,loc=0,scale = sigma* np.sqrt((df-2)/df))\n",
    "        else:\n",
    "            print(\"Distribution not known. Skipped to next distribution.\")\n",
    "            continue\n",
    "        \n",
    "        for n_trees in n_trees_vec:\n",
    "           \n",
    "            sups_uni=[]\n",
    "            sups_ehr=[]\n",
    "            sigma_hats=[]         \n",
    "\n",
    "            print(\"Simulation progress for error distribution \"+dist_name+\" with sigma =\",sigma,\" and n_trees =\",n_trees, \":\")\n",
    "            \n",
    "            results = Parallel(n_jobs=n_cpus)(\n",
    "                delayed(run_test)(i, n_samples, p, m_factor, regression_fct, delta, B, error_dist, rand_seed, n_trees, k, r, X_test, m_true_grid, h_g)\n",
    "                for i in tqdm(range(n_tests))\n",
    "            )\n",
    "            \n",
    "            # extract results\n",
    "            for sup_uni, sup_ehr, sigma_hat in results:\n",
    "                sups_uni.append(sup_uni)\n",
    "                sups_ehr.append(sup_ehr)\n",
    "                sigma_hats.append(sigma_hat)\n",
    "            \n",
    "            cover_num_uni=np.zeros(3)\n",
    "            cover_num_ehr=np.zeros(3)\n",
    "            for j in range(len(cb_rad_uni)):\n",
    "                cover_num_uni[j]=sum(np.array(sups_uni)<np.array(sigma_hats)*cb_rad_uni[j])\n",
    "                cover_num_ehr[j]=sum(np.array(sups_ehr)<np.array(sigma_hats)*cb_rad_ehr[j])\n",
    "            n_CB=len(sups_uni)\n",
    "            \n",
    "            avg_cb_rad_uni=cb_rad_uni*np.mean(sigma_hats)\n",
    "            avg_cb_rad_ehr=cb_rad_ehr*np.mean(sigma_hats)\n",
    "\n",
    "            if m_factor==1:\n",
    "                m_info = \" and m = \"+regression_fct.__name__\n",
    "            elif m_factor ==0:\n",
    "                m_info = \" and m set to zero.\"\n",
    "            else:\n",
    "                m_info = \" and m = \"+str(m_factor)+\"*\"+regression_fct.__name__\n",
    "            \n",
    "            result_txt = [\"\",\"\",\"Results  at \"+time.ctime(),\n",
    "                          \"for Random Seed = \"+str(rand_seed),\n",
    "                          \"and Regression model with:\",\n",
    "                          \"p = \"+str(p)+m_info,\n",
    "                          \"Parameters:\",\n",
    "                          \"Sample size: \"+str(n_samples),\n",
    "                          \"k=\"+str(k),\n",
    "                          \"r=\"+str(r*n_samples)]\n",
    "            if error_dist.dist.name == 't':\n",
    "                result_txt.append(\"Error distribution: t-Distribution with \"+str(df)+\" degrees of freedom\")\n",
    "            else:\n",
    "                result_txt.append(\"Error distribution: \" +str(error_dist.dist.name))\n",
    "            \n",
    "            result_txt.append(\"Error std: \"+str(sigma))\n",
    "            result_txt.append(\"Number of trees: \"+str(n_trees))\n",
    "            result_txt.append(\"Number of CBS: \"+str(n_CB))\n",
    "            result_txt+=[\"\",\"Empirical Coverage for confidence bands with theoretical coverage \"+ str(1-beta)+\":\"]\n",
    "            result_txt.append(\"Uniform CPRF, number: \"+str(cover_num_uni)+\", percentage: \"+ str(cover_num_uni/n_CB))\n",
    "            result_txt.append(\"Ehrenfest CPRF, number: \"+str(cover_num_ehr)+\", percentage: \"+ str(cover_num_ehr/n_CB))\n",
    "            result_txt+=[\"\",\"Average confidence band radius for theoretical coverage \"+ str(1-beta)+\":\"]\n",
    "            result_txt.append(\"Uniform CPRF: \"+str(avg_cb_rad_uni))\n",
    "            result_txt.append(\"Ehrenfest CPRF: \"+str(avg_cb_rad_ehr))\n",
    "            \n",
    "            # save results of combination in txt file\n",
    "            with open('Simulation results.txt', 'a') as f:\n",
    "                for line in result_txt:\n",
    "                    f.write(line + '\\n')\n",
    "            \n",
    "            print(\"Simulation for distribution \"+dist_name+\" with sigma =\",sigma,\" and n_trees =\",n_trees, \" complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
